[["index.html", "Portfolio Twan Samsom", " Portfolio Twan Samsom Wees welkom! Mijn naam is Twan Samsom en tijdens het volgen van mijn minor ‘Data sciences for biology’ heb ik o.a. gewerkt aan het maken van dit portfolio. In dit portfolio heb ik verschillende skills die ik heb opgedaan geraporteerd, het gross van de tekst heb ik in het Engels geschreven. Schroom niet om een kijkje te nemen, een goed idee is om te beginnen bij mijn Curriculum Vitae. Stukje uit mijn cv: Ik ben een enthousiaste derdejaarsstudent Life Sciences aan de Hogeschool Utrecht, met een sterke focus op biologie en data science. Ik heb ervaring opgedaan in diverse projecten, waar ik o.a. mijn data-analysevaardigheden inzet. 2023-12-23 "],["curriculum-vitae.html", "Chapter 1 Curriculum Vitae Opleidingen Werk Talen Vaardigheden", " Chapter 1 Curriculum Vitae Adres: ****** Telefoon: *********** Mail: Twan.samsom@student.hu.nl Geboortedatum: ********** Ik ben een enthousiaste derdejaarsstudent Life Sciences aan de Hogeschool Utrecht, met een sterke focus op biologie en data science. Ik heb ervaring opgedaan in diverse projecten, waar ik o.a. mijn data-analysevaardigheden inzet. Opleidingen Hogeschool Utrecht . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9/2023 – Heden Minor: Data Sciences for Biology Vakken: Data Analysis Using R, Advanced bioinformatica. Projecten Project RNA sequencing: Het effect van BCLXL en ONECUT1 op de genexpressie van fibroblasten, Beschrijving: Gebruik van R, R packages zoals: DESeq2 en ggplot, en het uitvoeren van een GO term enrichment analysis. Project Metagenomics: Identificatie van bacteriesoorten Beschrijving: Identificeren van bacteriën in een monster met gebruik van Kraken2 en Bracken voor metagenomics analyse. Hogeschool Utrecht . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9/2021 – Heden Biologie en Medisch Laboratoriumonderzoek (Life Sciences) . . . . . . . Verwachte afstudeerdatum: 8/2025 Vakken: Data Science, Statistiek &amp; Excel. Projecten Project Data Science: Het effect van PFOS op de genexpressie van ACOX1, Beschrijving: Onderzoek naar het effect van PFOS op genexpressie, met gebruik van BASH voor data-manipulatie en JASP voor statistiek. Project Farmacon: Het effect van Donepezil op kunstmatig geïnduceerde paralyse in CL4176 C. elegans Beschrijving: Onderzoek naar het effect van Donepezil op amyloïde plaquevorming. Gebruik van de ingebouwde ImageJ macrofunctie om een klein script te schrijven voor de automatisering van data-analysestappen. Propedeuse Hogeschool Utrecht . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Behaald in 2022 Biologie en Medisch Laboratoriumonderzoek (Life Sciences) Minkema College (havo) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9/2015 – 7/2021 Profiel N&amp;G / N&amp;T MBO Rijnland . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9/2020 – 7/2021 Certificaat Biologie VWO Werk Pizzeria Bella Milano, Woerden . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 04/2019 – Heden Opnemen en verwerken van bestellingen Bestellingen klaar maken en bezorgen Onderhouden van de website Talen Nederlands (Moedertaal) Engels (Vloeiend) Vaardigheden Programmeertalen R BASH Rapportage en Presentatie rmarkdown voor het maken van dynamische rapporten Data Analyse Ervaring met data-analyse en statistiek in R Gebruik van ggplot2 voor datavisualisatie Tekstmanipulatie Schrijven van BASH-scripts/pipelines Gebruik van AWK voor tekstmanipulatie Specifieke Libraries DESeq2: Differentiële expressie-analyse Kraken2: Identificatie van bacteriën in metagenomics tidyverse: Een verzameling van R-packages voor data science en data wrangling 2023-11-26 "],["learning-a-new-skill.html", "Chapter 2 Learning a New Skill 2.1 Planning 2.2 Planning after finding Dataset 2.3 Results 2.4 Evaluation", " Chapter 2 Learning a New Skill In this part I got 32 hours to do work on something I wanted to do or learn, I wanted to so something with machine learning as this is something very relevant. Ofcourse is 32 hours a bit short to learn about machine learning and create my own model so I decided to use an existing model, I settled with Sturgeon. 2.1 Planning I started by creating a planning. Day 1 Read the paper about sturgeon, “Ultra-fast deep-learned CNS tumour classification during surgery” Trying to find data that can be used Sequences of tumours in the CNS The data should be labeled so the results can be compared Data ideally needs to be nanopore data Installing required programs(depends on the data I found) Sturgeon - https://github.com/marcpaga/sturgeon Crossmap - https://github.com/liguowang/CrossMap Modkit - https://github.com/nanoporetech/modkit Guppy - https://github.com/nanoporetech/pyguppyclient Day 2 Using the needed tools to prepare the data for running sturgeon. Try to run sturgeon on a couple of files to see how long it would take to run on all 415 files. Day 3/4 Run sturgeon on more, perhaps all files. Gather the reference diagnosis for every file. Visualize the data. 2.2 Planning after finding Dataset I will be using the tool Sturgeon to clasify tumors found in the central nervous system. I will use the information in this paper “Ultra-fast deep-learned CNS tumour classification during surgery” and using this GSE209865 dataset from GEO. The data needs to be lifted using crossmap from the hg19 genome to the chm13v2.0 genome. 2.3 Results Everything I did is documented in this Rmarkdown. But I will share some results here as well. I ended up running the analysis on all the 415 files twice, one time with the lifted data and one without lifting the data. Let’s import the data. library(dplyr) library(ggplot2) library(DT) library(data.table) library(fs) output_table_matched &lt;- read.delim(&quot;sturgeon/sturgeon_dataframes/output_table_matched.tsv&quot;) output_table_matched_raw_data &lt;- read.delim(&quot;sturgeon/sturgeon_dataframes/output_table_matched_raw_data.tsv&quot;) Let’s check how many of the predictions were correct and how many were wrong. Here we see that 365 of the 415 predictions were correct that’s around 88% But sturgeon gives a confidence score the following is written in the README about the confidence score. score &lt; 0.8: inconclusive result. 0.8 &lt;= score &lt; 0.95: confident result that the class is correct. score &gt;= 0.95: high confident result that the class is correct. Lets look at only the predictions that got a score above 0.8. The amount of wrong predictions almost halved, of the predictions with a confidence score above 0.8 about 92.5% were correct. And then also look at the predictions that have a high confident result &gt; 0.95 The predictions with a confidence score above 0.95 are even more accurate with about 94,7% accuracy. Lets take a look at the wrong predictions, lets see which reference diagnosis were guessed wrong the most The data in the graph is normalized so bars that have a normalized count of 1 were never guessed correctly, but most of these only occur once. Glioblastoma, IDH-wildtype is guessed wrong most often but appears a hundred times in the dataset so its still guessed right more than 80% of the time. It’s also very much possible that some of the predictions that are marked as wrong might be correct but that something went wrong when translating the prediction to the reference_diagnosis form. I noticed that the .bed files differed quite a lot in size, so i wondered whether confidence score or maybe the amount of correct prediction increased when the size of the file was bigger. Scatterplot file size vs confidence score There is a small correlation between file size and confidence score, The predictions with very low confidence score are always small files. Lets take a look at the predictions from the raw_data run. The amount of wrong predictions are exactly the same as the amount of wrong predictions when using the lifted files. But maybe the predictions are made with less confidence ## [1] -0.0006995533 The mean confidence scores of the two runs are very close to each other and the raw_data mean is even a little higher. Lets look at the distribution of the confidence scores The two violin plots look almost identical, running sturgeon without lifting the genome doesn’t change much about the confidence score. Let’s see if there were instances where there was something different guessed between the two runs. ## GEO_accession reference_diagnosis.x matched_reference_diagnosis ## 48 GSM6402724 Ependymoma Pleomorphicxanthoastrocytoma ## 111 GSM6402787 Glioblastoma,IDH-wildtype Glioblastoma,IDH-wildtype ## matched_reference_diagnosis_raw ## 48 Ependymoma ## 111 Glioblastoma,IDH-mutant As we can see there are two instances where a different guess was made, once the run with the raw_files guessed the wrong Glioblastoma type and once the run with lifted files geussed the Ependymoma wrong. lifting the genome to CHM13v2 doesn’t seem that important at least not when working with data aligned to the hg19 genome. 2.4 Evaluation I am very satisfied with the result of this analysis, it was a lot of fun to come up with my own ideas for what to do. And it was very satisfying to find solution on the problems I was facing. I don’t think I necessarily learned a specific new skill, but I certainly learned new things from this experience, whilst navigating my way around this analysis and the different tools, and problem solving the problem that I encountered. "],["looking-into-reproducible-science.html", "Chapter 3 Looking into Reproducible Science 3.1 Per‐ and polyfluoroalkyl substances activate UPR pathway, induce steatosis and fibrosis in liver cells 3.2 Hormonal correlates of pathogen disgust: testing the compensatory prophylaxis hypothesis", " Chapter 3 Looking into Reproducible Science In this part we will look into reproducibilty of scientific publications, we will be doing this by comparing a paper to the criteria that is available on Researchgate, and by trying to recreate a part of the analysis from an different paper. 3.1 Per‐ and polyfluoroalkyl substances activate UPR pathway, induce steatosis and fibrosis in liver cells https://doi.org/10.1002/tox.23680 Qi, Q., Niture, S., Gadi, S., Arthur, E., Moore, J., Levine, K. E., &amp; Kumar, D. (2023) The article researches how the retention of fat, fibrogenic signals and cell survival are influenced by low concentrations of three different PFAS in liver cell models. This is done using a handfull of techniques like cell counting and staining, RT qPCR, RNA-seq and others. Important findings are that PFAS in low concentrations can cause ER stress, steatosis and fibrogenic signaling, therefore associating PFAS to the development of non alcoholic fatty liver disease. We will score this paper on the following criteria: Study Purpose, Data Availability Statement, Data Location, Study Location, Author Review, Ethics Statement, Funding Statement and on Code Availability. This is done by filling in the table provided on Researchgate. The table seen beneath is the table with criteria filled in for the study we chosen above. Transparency Criteria Definition Response Type Score Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective. Binary no Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. Binary no Data Location Where the article’s data can be accessed, either raw or processed. Found Value ArrayExpress E‐MTAB‐11670, cell photographs are linked at the top under “Supplementary Materials” Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. Binary; Found Value no Author Review The professionalism of the contact information that the author has provided in the manuscript. Found Value Tier 3 Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. Binary no Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. Binary yes Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. Binary no Data Availablilty statement is score as a no, but at the beginning of the paper there is a small paragraph called Supplementary Materials where the cell photographs are linked. But there is no part about the RNA-seq data or the qPCR data. The RNA-seq data can be found hidden at the end of the methods. There is a clear statement about funding: Funding information National Institutes of Health, Grant/Award Numbers: R01MD012767, U54MD012392, U01CA194730. There is no ethics statement and nothing is said about code used. 3.2 Hormonal correlates of pathogen disgust: testing the compensatory prophylaxis hypothesis https://doi.org/10.1016/j.evolhumbehav.2017.12.004. Benedict C. Jones, Amanda C. Hahn, Claire I. Fisher, Hongyi Wang, Michal Kandrik, Anthony J. Lee, Joshua M. Tybur, Lisa M. DeBruine(2018) The paper investigates if elevated progesterone during the menstrual cycle, leads to a higher pathogen disgust. The code and data used in this paper can be found here. We will first take a look at the code, all the code is stored in the OCMATE_disgust2.RMD file. First they load the data into R using read_csv, then they calculate some simple stats like mean age and how many session completed per woman. Then they clean up the data by removing points with missing values, excluding subjects with only one session completed and removing outliers. After that some plots and a lot of tables are formed. There is not really an introduction, they dive straight into loading libraries and importing code. There are some comments in the code but they are very basic and tell very little about the code. The code is structured in chunks with most chunks having a short header stating the goal of the code. Overall i would say that the readability of the code is a 2/5. Now we will try to run the Rmarkdown file ourselves, the first time trying to knit the Rmd we encountered some issues because two packages were not installed. We installed the packages using the following lines. install.packages(&quot;Matrix&quot;, dependencies = TRUE) install.packages(&quot;lme4&quot;, dependencies = TRUE) And after installing the packages the Rmd could be knitted, the entire html file can be seen here. But i will share a plot here as well. This is the first plot in the html file, very little is said about this plot except “# subject-mean-centre hormones # and divide by a constant to put all hormones on ~ -0.5 to +0.5 scale.” It took very little effort for me to reproduce the visualizations of this paper so I will rank it 5/5 on how much effort it took to reproduce. "],["sql.html", "Chapter 4 SQL 4.1 Getting the data 4.2 Tidy the data 4.3 Using SQL and R to inspect data 4.4 Summarzing flu and dengue 4.5 Merging the tables 4.6 Creating some plots", " Chapter 4 SQL N.B. some of the steps are done using an connection to a local postgres database, because these wouldn’t work in an R markdown they are replaced by screenshots. 4.1 Getting the data First we need to get the data for the exercise, we use wget to download them from github. # Go to data_raw cd sql/raw_data/ # Download the flu dataset wget -O flu_data.csv https://raw.githubusercontent.com/DataScienceILC/tlsc-dsfb26v-20_workflows/main/data/flu_data.csv # Download the dengue dataset wget -O dengue_data.csv https://raw.githubusercontent.com/DataScienceILC/tlsc-dsfb26v-20_workflows/main/data/dengue_data.csv 4.2 Tidy the data Its important to make the data tidy so every table matches structure wise, in these steps we also make some changes to make merging later easier. #load needed libraries library(tidyr) library(dplyr) library(RPostgreSQL) library(dslabs) # Load the different data flu_data &lt;- read.csv(&quot;sql/raw_data/flu_data.csv&quot;, skip = 11) # skip = 11 to skip the first 11 lines which contain metadata dengue_data &lt;- read.csv(&quot;sql/raw_data/dengue_data.csv&quot;, skip = 11) gapminder_data &lt;- as_tibble(gapminder) ## flu_data # Make flu_data tidy tidy_flu_data &lt;- flu_data %&gt;% pivot_longer(cols = -Date, names_to = &quot;country&quot;, values_to = &quot;cases&quot;) # Split Date into year, month, day tidy_flu_data &lt;- tidy_flu_data %&gt;% separate(Date, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), sep = &quot;-&quot;) # Convert year into factor tidy_flu_data$year &lt;- as.factor(tidy_flu_data$year) # flu_data uses dots instead of spaces in country names (New.Zealand) # So to make joining the tables easier we need to replace the dots with spaces tidy_flu_data$country &lt;- gsub(&quot;\\\\.&quot;, &quot; &quot;, tidy_flu_data$country) # turn country into factor tidy_flu_data$country &lt;- as.factor(tidy_flu_data$country) # dengue_data # Make dengue_data tidy tidy_dengue_data &lt;- dengue_data %&gt;% pivot_longer(cols = -Date, names_to = &quot;country&quot;, values_to = &quot;cases&quot;) # Split Date into year, month, day tidy_dengue_data &lt;- tidy_dengue_data %&gt;% separate(Date, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), sep = &quot;-&quot;) # Convert year into factor tidy_dengue_data$year &lt;- as.factor(tidy_dengue_data$year) # turn country into factor tidy_dengue_data$country &lt;- as.factor(tidy_dengue_data$country) # Save tidy_dengue_data write.csv(tidy_dengue_data, &quot;sql/data/tidy_dengue_data.csv&quot;) saveRDS(tidy_dengue_data, &quot;sql/data/tidy_dengue_data.rds&quot;) # Save tidy_flu_data write.csv(tidy_flu_data, &quot;sql/data/tidy_flu_data.csv&quot;) saveRDS(tidy_flu_data, &quot;sql/data/tidy_flu_data.rds&quot;) # Save clean gapminder_data write.csv(gapminder_data, &quot;sql/data/tidy_gapminder_data.csv&quot;) saveRDS(gapminder_data, &quot;sql/data/tidy_gapminder_data.rds&quot;) The chunck beneath is to create an connection object, that can be used to connect to a Postgres database. # Load packages library(RPostgreSQL) library(DBI) # Create connection object to connect to local PostgreSQL database con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflowsdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;PASSWORD&quot;) This chunck would normaly create tables in the postgres database. # Load tidy_flu_data into Postgres database dbWriteTable(con, &quot;tidy_flu_data&quot;, as.data.frame(tidy_flu_data), overwrite = TRUE) # Load tidy_dengue_data into Postgres database dbWriteTable(con, &quot;tidy_dengue_data&quot;, as.data.frame(tidy_dengue_data), overwrite = TRUE) # Load gapminder_data into Postgres database dbWriteTable(con, &quot;gapminder_data&quot;, as.data.frame(gapminder_data), overwrite = TRUE) 4.3 Using SQL and R to inspect data We use both SQL and R to inspect the data in the same way 4.3.1 Gapminder Here we order the gapminder data by life expactancy. Using R with the same goal. gapminder_data %&gt;% arrange(desc(life_expectancy)) ## # A tibble: 10,545 × 9 ## country year infant_mortality life_expectancy fertility population gdp ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Hong Kong,… 2016 NA 83.9 NA NA NA ## 2 Hong Kong,… 2015 NA 83.7 1.17 7287983 NA ## 3 Hong Kong,… 2014 NA 83.6 1.15 7226869 NA ## 4 Hong Kong,… 2013 NA 83.4 1.14 7163930 NA ## 5 Iceland 2014 1.6 83.3 2.07 327318 NA ## 6 Iceland 2015 1.6 83.3 2.06 329425 NA ## 7 Iceland 2016 NA 83.3 NA NA NA ## 8 Japan 2016 NA 83.3 NA NA NA ## 9 Hong Kong,… 2012 NA 83.2 1.12 7101858 NA ## 10 Iceland 2013 1.6 83.2 2.08 325392 NA ## # … with 10,535 more rows, and 2 more variables: continent &lt;fct&gt;, region &lt;fct&gt; 4.3.2 Flu Here we select only the cases in France from the flu data. Again using R for the same goal. tidy_flu_data %&gt;% filter(tolower(country) == &#39;france&#39; &amp; !is.na(cases)) %&gt;% arrange(year, month, day) ## # A tibble: 620 × 5 ## year month day country cases ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;int&gt; ## 1 2003 09 28 France 25 ## 2 2003 10 05 France 25 ## 3 2003 10 12 France 30 ## 4 2003 10 19 France 81 ## 5 2003 10 26 France 82 ## 6 2003 11 02 France 48 ## 7 2003 11 09 France 146 ## 8 2003 11 16 France 274 ## 9 2003 11 23 France 877 ## 10 2003 11 30 France 1282 ## # … with 610 more rows 4.3.3 Dengue Here we sort dengue by cases from high to low. And again R for the same goal. tidy_dengue_data %&gt;% filter(!is.na(cases)) %&gt;% arrange(desc(cases)) ## # A tibble: 6,263 × 5 ## year month day country cases ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 2004 02 29 Indonesia 1 ## 2 2005 09 11 Singapore 1 ## 3 2008 04 06 Brazil 1 ## 4 2009 02 08 Bolivia 1 ## 5 2009 05 03 Argentina 1 ## 6 2009 09 27 Mexico 1 ## 7 2010 05 30 Venezuela 1 ## 8 2010 08 29 Philippines 1 ## 9 2012 10 21 India 1 ## 10 2013 07 14 Thailand 1 ## # … with 6,253 more rows Here you can see that in every country, the highest case is 1 because the data is relative per country. The day where there were the most Google searches about dengue is labeled as one, and the day with the fewest is labeled as 0. Every other day is on the scale between those. 4.4 Summarzing flu and dengue Both the flu and dengue data have datapoints every day, while gapminder has a datapoint for every year. Before we can join flu and dengue with gapminder we first have to summarize the data so flu and dengue to have a datapoint for every year. For flu we can just take the total of every case, hence we use sum in the summarize function. summarized_flu_data &lt;- tidy_flu_data %&gt;% mutate(Date = as.Date(paste(year, month, day, sep = &quot;-&quot;))) %&gt;% group_by(country, year) %&gt;% summarize(total_cases_flu = sum(cases, na.rm = TRUE)) %&gt;% ungroup() For dengue, we need to calculate the average because the data is on a scale where 1 represents the highest search activity in a country, and 0 represents the lowest. Therefore, we use the mean function in the summarize function. summarized_dengue_data &lt;- tidy_dengue_data %&gt;% mutate(Date = as.Date(paste(year, month, day, sep = &quot;-&quot;))) %&gt;% group_by(country, year) %&gt;% summarize(avg_cases_dengue = mean(cases, na.rm = TRUE)) %&gt;% ungroup() Now we load the new summarized files into the database. # Load summarized_flu_data dbWriteTable(con, &quot;summarized_flu_data&quot;, as.data.frame(summarized_flu_data), overwrite = TRUE) # Load summarized_dengue_data dbWriteTable(con, &quot;summarized_dengue_data&quot;, as.data.frame(summarized_dengue_data), overwrite = TRUE) 4.5 Merging the tables Normally this could be used to merge the tables and load the table into R. # SQL to merge tables sql_query &lt;- &quot; SELECT COALESCE(d.country, f.country) AS merged_country, COALESCE(d.year, f.year::text) AS merged_year, d.avg_cases_dengue, f.total_cases_flu, g.* FROM summarized_dengue_data AS d FULL JOIN summarized_flu_data AS f ON d.year = f.year::text AND d.country = f.country JOIN gapminder_data AS g ON COALESCE(d.year, f.year::text) = g.year::text AND COALESCE(d.country, f.country) = g.country ORDER BY COALESCE(d.avg_cases_dengue, 0) DESC, COALESCE(f.total_cases_flu, 0) DESC; &quot; # Export table to R merged_table &lt;- dbGetQuery(con, sql_query) summary(merged_table) Because we can’t use SQL we just load the merged table in using R, and create a summary of the data. merged_table &lt;- readRDS(&quot;sql/data/merged_table.rds&quot;) summary(merged_table) ## merged_country merged_year avg_cases_dengue total_cases_flu ## Length:490 Length:490 Min. :0.0108 Min. : 0 ## Class :character Class :character 1st Qu.:0.0796 1st Qu.: 1696 ## Mode :character Mode :character Median :0.1216 Median : 6772 ## Mean :0.1363 Mean : 20147 ## 3rd Qu.:0.1772 3rd Qu.: 24975 ## Max. :0.5355 Max. :155577 ## NA&#39;s :360 NA&#39;s :84 ## country year infant_mortality life_expectancy ## Length:490 Min. :2002 Min. : 2.00 Min. :52.50 ## Class :character 1st Qu.:2005 1st Qu.: 4.00 1st Qu.:73.20 ## Mode :character Median :2008 Median : 7.60 Median :76.50 ## Mean :2008 Mean :12.61 Mean :75.84 ## 3rd Qu.:2012 3rd Qu.:15.88 3rd Qu.:80.50 ## Max. :2015 Max. :62.00 Max. :83.20 ## ## fertility population gdp continent ## Min. :1.150 Min. :3.324e+06 Min. :7.214e+09 Length:490 ## 1st Qu.:1.430 1st Qu.:9.241e+06 1st Qu.:8.430e+10 Class :character ## Median :1.850 Median :3.001e+07 Median :2.193e+11 Mode :character ## Mean :1.945 Mean :8.865e+07 Mean :7.992e+11 ## 3rd Qu.:2.288 3rd Qu.:8.051e+07 3rd Qu.:5.853e+11 ## Max. :3.980 Max. :1.311e+09 Max. :1.174e+13 ## NA&#39;s :141 ## region ## Length:490 ## Class :character ## Mode :character ## ## ## ## 4.6 Creating some plots We now use the merged table to create some plots to visualize the data. Create boxplot of life expectancy per continent. library(ggplot2) # Boxplot of Life Expectancy by Continent ggplot(merged_table, aes(x = continent, y = life_expectancy, fill = continent)) + geom_boxplot() + labs(title = &quot;Boxplot of Life Expectancy by Continent&quot;, x = &quot;Continent&quot;, y = &quot;Life Expectancy&quot;) + theme_minimal() + scale_fill_discrete(name = &quot;Continent&quot;) Linegraph for dengue cases in Brazil, Argentina and Bolivia. # chosen 3 random countries selected_countries &lt;- c(&quot;Brazil&quot;, &quot;Argentina&quot;, &quot;Bolivia&quot;) filtered_data_dengue &lt;- merged_table[merged_table$country %in% selected_countries, ] # Line plot of Dengue cases for Brazil, Argentina and Bolivia ggplot(filtered_data_dengue, aes(x = year, y = avg_cases_dengue, color = country)) + geom_line() + labs(title = &quot;Dengue Cases Over the Years (Selected Countries)&quot;, x = &quot;Year&quot;, y = &quot;Average Dengue Cases&quot;, color = &quot;Country&quot;) + theme_minimal() ## Warning: Removed 1 row containing missing values (`geom_line()`). Flu cases per capita per country. # Filter data to exclude NA values filtered_data &lt;- merged_table %&gt;% filter(!is.na(total_cases_flu) &amp; !is.na(population)) # Divide the cases by the population filtered_data$flu_ratio &lt;- filtered_data$total_cases_flu / filtered_data$population # Create a box plot ggplot(filtered_data, aes(x = merged_country, y = flu_ratio)) + geom_boxplot() + labs(title = &quot;Distribution of Flu Cases per Population per Country&quot;, x = &quot;Country&quot;, y = &quot;Flu Cases per Population&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Make readable In this graph we can see in what countries Flu is most common. "],["reproducing-an-analysis.html", "Chapter 5 Reproducing an Analysis 5.1 Getting the data 5.2 Normalizing the data 5.3 Visualizing the data 5.4 Normalizing the data again 5.5 Visualizing the data again 5.6 Step-wise Plan for Dose-Response Curve Analysis", " Chapter 5 Reproducing an Analysis In this part we will be reproducing an analysis with data supplied by J. Louter (INT/ILC) Different compounds were tested on adult C.elegans and the amount of offspring were counted. # Load libraries library(readr) library(ggplot2) library(DT) library(readxl) library(dplyr) 5.1 Getting the data First we will need to get the data from somewhere, we use wget to download the excel file from github. # Go to raw_data cd c.elegans/raw_data/ # Download the excel sheet dataset wget -O &quot;CE.LIQ.FLOW.062_Tidydata.xlsx&quot; https://github.com/DataScienceILC/tlsc-dsfb26v-20_workflows/raw/main/data/CE.LIQ.FLOW.062_Tidydata.xlsx Now we can read the file into R. # Read the excel file C.elegans_data &lt;- read_excel(path = &quot;c.elegans/raw_data/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;) The file is now read into R. 5.2 Normalizing the data # Display the important rows C.elegans_data %&gt;% select(c(RawData, compName, compConcentration)) ## # A tibble: 360 × 3 ## RawData compName compConcentration ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 44 2,6-diisopropylnaphthalene 4.99 ## 2 37 2,6-diisopropylnaphthalene 4.99 ## 3 45 2,6-diisopropylnaphthalene 4.99 ## 4 47 2,6-diisopropylnaphthalene 4.99 ## 5 41 2,6-diisopropylnaphthalene 4.99 ## 6 35 2,6-diisopropylnaphthalene 4.99 ## 7 41 2,6-diisopropylnaphthalene 4.99 ## 8 36 2,6-diisopropylnaphthalene 4.99 ## 9 40 2,6-diisopropylnaphthalene 4.99 ## 10 38 2,6-diisopropylnaphthalene 4.99 ## # … with 350 more rows Here we see that compConcentration is of chr type, so we have to change it into a numeric type. # Use parse_number to change the conecentration into numeric C.elegans_data$compConcentration &lt;- parse_number(C.elegans_data$compConcentration) # Display the rows again C.elegans_data %&gt;% select(c(RawData, compName, compConcentration)) ## # A tibble: 360 × 3 ## RawData compName compConcentration ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 44 2,6-diisopropylnaphthalene 4.99 ## 2 37 2,6-diisopropylnaphthalene 4.99 ## 3 45 2,6-diisopropylnaphthalene 4.99 ## 4 47 2,6-diisopropylnaphthalene 4.99 ## 5 41 2,6-diisopropylnaphthalene 4.99 ## 6 35 2,6-diisopropylnaphthalene 4.99 ## 7 41 2,6-diisopropylnaphthalene 4.99 ## 8 36 2,6-diisopropylnaphthalene 4.99 ## 9 40 2,6-diisopropylnaphthalene 4.99 ## 10 38 2,6-diisopropylnaphthalene 4.99 ## # … with 350 more rows We can see the type is now of dbl type. 5.3 Visualizing the data # Create scatterplot ggplot(C.elegans_data, aes(x = compConcentration, y = RawData, color = compName, shape = expType)) + geom_point() + labs(x = &quot;Compound Concentration (nM)&quot;, y = &quot;Raw Data Counts&quot;) ## Warning: Removed 5 rows containing missing values (`geom_point()`). The graph is really cramped into the left part because of a couple of very high concentrations. This occurs because the concentrations are created by diluting the previous solution by 1/10, so the concentrations are exponential. An obvious fix for this is to use an log10 transformation on the x-axis. # Create plot, note that we added + 0.01 this is becasue the log10 of 0 is undefined. # So adding 0.01 makes sure that points where concentration is 0 (controlNegative) show up in the graph ggplot(C.elegans_data, aes(x = log10(compConcentration + 0.01), y = RawData, color = compName, shape = expType)) + geom_point(position = position_jitter(width = 0.2, height = 0.2)) + # Add jitter labs(x = &quot;Log10(Compound Concentration (nM))&quot;, y = &quot;Raw Data Counts&quot;) ## Warning: Removed 5 rows containing missing values (`geom_point()`). This looks way better, we also added some jitter to prevent points from overlapping. 5.4 Normalizing the data again We have put nanomolar(nM) on the x axis but not all the data is in nM some points are in percentage(pct) as shown below #show that both nM and pct are used C.elegans_data$compUnit %&gt;% unique() ## [1] &quot;nM&quot; &quot;pct&quot; Lets take a look to see where pct is being used. # Select rows where compUnit is pct and select the more important columns C.elegans_data %&gt;% filter(compUnit == &quot;pct&quot;) %&gt;% select(c(compName, expType, RawData, compConcentration, compUnit)) %&gt;% datatable(options = list(pageLength = 5)) # Show as datatable As you can see in the interactive table both the positive control and the negative control, as well as the vehicle control, are in percentage (pct). Since the concentration (pct) of the negative control is 0 (nothing added) we can ignore the negative control and focus on the other two, these are both ethanol solution with either 1,5% or 0,5% ethanol. Both the positive control and the control Vehicle A are in percentage ethanol solution. We can use a formula to transfer these to nM like the rest of the data. \\[nM=(x*10^7)/46,07\\] Where x is the percentage ethanol. Here we use this formula to change the percentages to molarity but its important to do this only in the rows where the compName is ethanol and the compunit pct. # Change compConcentration where compName is Ethanol and compUnit pct C.elegans_data$compConcentration &lt;- ifelse(C.elegans_data$compName == &quot;Ethanol&quot; &amp; C.elegans_data$compUnit == &quot;pct&quot;, (C.elegans_data$compConcentration * 10^7) / 46.07, C.elegans_data$compConcentration) Now that we changed the values from percentage to nM we can visualize the data again. 5.5 Visualizing the data again Now the data is acctually normalized we can finally visualize the data so we create a scatterplot again. ggplot(C.elegans_data, aes(x = log10(compConcentration + 0.01), y = RawData, color = compName, shape = expType)) + geom_point(position = position_jitter(width = 0.2, height = 0.2)) + # Add jitter labs(x = &quot;Log10(Compound Concentration)&quot;, y = &quot;Raw Data Counts&quot;) ## Warning: Removed 5 rows containing missing values (`geom_point()`). We also create a faceted graph to make it a bit easier to see the different controls. # Create a faceted scatterplot with log-transformed x-axis and jitter using crosses ggplot(C.elegans_data, aes(x = log10(compConcentration + 0.01), y = RawData, color = compName)) + geom_point(shape = &quot;+&quot;, size = 3, position = position_jitter(width = 0.2, height = 0.2)) + labs(x = &quot;Log10(Compound Concentration)&quot;, y = &quot;Raw Data Counts&quot;) + facet_wrap(~expType, scales = &quot;fixed&quot;, ncol = 2) + # split graph in 4 for every exptype theme_minimal() ## Warning: Removed 5 rows containing missing values (`geom_point()`). The negative control for this experiment is S-medium 0 nM so basically nothing is added, and the positive control is a high concentration ethanol. Normalizing the data for the negative control helps compare different compounds fairly. It removes baseline differences and it makes it easier to see the specific effects of each compound on offspring count. Normalize data so the mean of the negative control is 1. # Find the mean of controlNegative controlNegative_mean &lt;- mean(C.elegans_data$RawData[C.elegans_data$expType == &quot;controlNegative&quot;], na.rm = TRUE) # Use mean to nomralize data C.elegans_data &lt;- C.elegans_data %&gt;% mutate(Normalized_RawData = RawData / controlNegative_mean) ggplot(C.elegans_data, aes(x = log10(compConcentration + 0.01), y = Normalized_RawData, color = compName, shape = expType)) + geom_point(position = position_jitter(width = 0.2, height = 0.2)) + labs(x = &quot;Log10(Compound Concentration)&quot;, y = &quot;Normalized Raw Data&quot;) ## Warning: Removed 5 rows containing missing values (`geom_point()`). 5.6 Step-wise Plan for Dose-Response Curve Analysis Usually after normalizing the results like this, dose response curves are made to assess whether different concentrations have a different effect on the offspring count, and to find out how the curves from the different compounds compare to each other. One way to do this is using the {drc} package in R I used code, for the steps using the drc package from, https://github.com/RaoulWolf/FMI330 Step 1, Fitting Dose-Response Curves First we should use the {drm} function from the {drc} package to fit a dose-response model for each compound. This would look something like this: fronds_number_inhibition.drm &lt;- drm(formula = Normalized_RawData ~ log10(compConcentration + 0.01), data = C.elegans_data, fct = LL.4(names = c(&quot;Slope&quot;, &quot;Lower Limit&quot;, &quot;Upper Limit&quot;, &quot;EC50&quot;))) The above code doesn’t work right now but it should look something like that. Step 2, Plotting the Dose-Response Curves Now we can plot the Dose-Response Curves data.frame(compConcentration = seq(from = min(C.elegans_data$compConcentration), to = max(C.elegans_data$compConcentration), length.out = 1000)) %&gt;% mutate(fit = predict(fronds_number_inhibition.drm, newdata = .), lwr = predict(fronds_number_inhibition.drm, newdata = ., interval = &quot;confidence&quot;, vcov. = sandwich)[, 2], upr = predict(fronds_number_inhibition.drm, newdata = ., interval = &quot;confidence&quot;, vcov. = sandwich)[, 3]) %&gt;% ggplot() + geom_ribbon(mapping = aes(x = compConcentration, ymin = lwr, ymax = upr), alpha = 0.2) + geom_line(mapping = aes(x = compConcentration, y = fit), size = 1) + geom_point(mapping = aes(x = compConcentration, y = Normalized_RawData), data = C.elegans_data, size = 2, alpha = 0.5) + labs(x = &quot;Diuron (mg/L)&quot;, y = &quot;Fronds number inhibition (%)&quot;) + theme_light() Step 3, Estimating LC50 To complete the LC50 analysis, we need to extract LC50 (lethal concentration for 50% inhibition). lc50 &lt;- ED(fronds_number_inhibition.drm, 50) Step 4, ANOVA As the last stap we would want to test for significant differences between the compunds this could be done with an ANOVA. "],["introduction-to-projecticum.html", "Chapter 6 Introduction to projecticum 6.1 Introductie 6.2 Plasmiden 6.3 One Health 6.4 Het Project", " Chapter 6 Introduction to projecticum Naast de lessen van workflows heb ik ook aan een project gewerkt, hieronder staat de introductie van dit project. De bronnen zijn bijgehouden en verwezen met behulp van zotero. 6.1 Introductie Het toenemen van antibiotica resistentie is een zorgwekkend probleem wereldwijd. Jaarlijks worden er 1,2 miljoen sterfgevallen veroorzaakt door antibiotica resistentie (Murray et al. 2022). Antibioticaresistentie treedt op wanneer bacteriën zich aanpassen en ongevoelig worden voor de effecten van antibiotica, waardoor deze geneesmiddelen minder effectief worden bij de behandeling van bacteriële infecties. 6.2 Plasmiden Een belangrijke factor in de verspreiding van antibioticaresistentie zijn plasmiden, kleine cirkelvormige stukjes DNA die naast het chromosomale DNA in bacteriën voorkomen. Deze plasmiden kunnen verschillende genen bevatten, waaronder genen die coderen voor antibioticaresistentie. De plasmiden kunnen niet alleen door het delen van de bacteriën doorgegeven worden, maar ook via ‘horizontal gene transfer’ waarbij plasmiden worden overgedragen van de ene bacterie naar de andere. (Harrison and Brockhurst 2012) Dit stelt bacteriën in staat om snel genetisch materiaal uit te wisselen, waardoor de resistentie tegen antibiotica zich gemakkelijk kan verspreiden tussen verschillende bacteriële populaties, meestal naar bacteriën van de zelfde soort maar kan ook plaats vinden tussen verschillende soorten (Dahlberg et al. 1998). 6.3 One Health Volgens het WHO zijn de grootste drijfveren van antibiotica resistentie is het overmatig gebruik van antibiotica in zowel mensen als in de landbouw (“Antimicrobial Resistance” n.d.). Een initiatief om de antibioticaresistentie aan te pakken is het One Health aanpak. One Health is een verzameling van 10 instanties o.a. verschillende universiteiten medisch centra en het RIVM. De One Health-aanpak, richt zich op het gezamenlijk aanpakken van gezondheidsproblemen op het vlak van menselijke gezondheid, dierlijke gezondheid en milieu. Om de verspreiding van resistente micro-organismen te beheersen en nieuwe infecties te voorkomen “One Health | RIVM” (n.d.). 6.4 Het Project Twee medestudenten en ik hebben de kans gekregen om de skills die we hebben opgebouwd tijdens de cursus ‘Datascience for biology’ toe te passen in een project. Het project waar wij aan mogen werken is een project met een externe opdracht gever van het RIVM. Voor ons project bouwen we verder aan het werk van een vorige groep, die hebben een pipeline gemaakt in R en BASH, gerapporteerd in een Rmarkdown. Met deze pipeline kan vanuit .fasta sequenties van bacteriën informatie gewonnen worden over de antibioticaresistentie genen op de plasmiden die in deze bacteriën aanwezig zijn. Deze bedoeling van deze pipeline is dat deze duizenden sequenties van bacteriën kan gaan analyseren. Wij hebben diversen doelen voor het project, zo voegen wij verschillende tools toe aan de pipeline om meer informatie op te doen over de plasmiden. We clusteren de plasmiden die veel op elkaar lijken, we bepalen a.d.h.v. de plasmide sequenties de mobiliteit van de plasmiden, we bepalen het replicontypes van de plasmiden en tot slot nog de virulentiegenen. Voor het verkrijgen van deze informatie worden veel tools gebruikt, en al deze tools rapporteren de resultaten op andere manieren, een belangrijke taak in ons project is dus ook het verzamelen van deze resultaten van deze tools en dit overzichtelijk rapporteren in een metadata bestand. Nieuwe sequenties moeten geanalyseerd kunnen worden door de pipeline en de nieuwe data moet in het metadata toegevoegd worden. Daarnaast gaan we ook een Rshiny maken waarin we de data visualiseren, we willen deze Rshiny zo gaan maken dat deze interactief is . "],["r-package.html", "Chapter 7 R package", " Chapter 7 R package I created an R package {CovidReportECDC}, the package can be installed using devtools. The package contains three functions and can be used to make plots about covid. The R package can be found on my github. I will show an example of how it could be used bellow. devtools::install_github(&quot;twananas/CovidReportECDC&quot;) library(CovidReportECDC) plot_covid(country = &quot;Netherlands&quot;, 2022, c(1,2,3) ) COVID-19 cases in nederland 1-3/2022 "],["guerrilla-analytics-framework.html", "Chapter 8 Guerrilla Analytics Framework", " Chapter 8 Guerrilla Analytics Framework During the lessons workflows I learned to work following the Guerrilla analytics framework principles. We learned multiple things here such as Working in Github for version control. Automating things. And maintaning an organized folder structure. Here I ordered the folder structure from a fellow classmates daur2 folders, I divided everthing into seperate projects te keep everthing clear and easy to find. fs::dir_tree(&quot;daur2_correct_structure&quot;) ## daur2_correct_structure ## ├── README.md ## ├── RNA_sequencing ## │ ├── RNA_sequencing.Rmd ## │ ├── RNA_sequencing.html ## │ ├── data ## │ ├── raw_data ## │ └── results ## ├── eind_opdracht ## │ ├── RNA_sequencing_eind_opdracht.Rmd ## │ ├── RNA_sequencing_eind_opdracht.html ## │ ├── data ## │ ├── raw_data ## │ └── results ## ├── formatieve_toets ## │ ├── data ## │ ├── formatieve_toets.Rmd ## │ ├── formatieve_toets.html ## │ ├── raw_data ## │ └── results ## ├── lessen ## │ ├── les1 ## │ │ ├── les1.R ## │ │ ├── les1.Rmd ## │ │ ├── les1.html ## │ │ └── les1.sh ## │ └── les2 ## │ ├── les2.Rmd ## │ └── les2.html ## ├── metagenomics ## │ ├── bracken ## │ ├── data ## │ ├── kraken ## │ ├── metagenomics.Rmd ## │ ├── metagenomics.html ## │ ├── raw_data ## │ └── results ## └── rnaseq_ipsc ## ├── data ## │ └── read_counts.rds ## ├── raw_data ## │ └── ipsc_sampledata.csv ## ├── rnaseq_ipsc.Rmd ## └── rnaseq_ipsc.html "],["refrences.html", "Chapter 9 Refrences", " Chapter 9 Refrences "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
