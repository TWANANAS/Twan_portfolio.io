[["index.html", "Portfolio Twan Chapter 1 About", " Portfolio Twan Twan Samsom 2023-12-10 Chapter 1 About This is my portfolio written in Markdown. "],["curriculum-vitae.html", "Chapter 2 Curriculum Vitae 2.1 Opleidingen 2.2 Werk 2.3 Talen 2.4 Vaardigheden", " Chapter 2 Curriculum Vitae Adres: ****** Telefoon: *********** Mail: Twan.samsom@student.hu.nl Geboortedatum: ********** Ik ben een enthousiaste derdejaarsstudent Life Sciences aan de Hogeschool Utrecht, met een sterke focus op biologie en data science. Ik heb ervaring opgedaan in diverse projecten, waar ik o.a. mijn data-analysevaardigheden inzet. 2.1 Opleidingen Hogeschool Utrecht . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9/2023 – Heden Minor: Data Sciences for Biology Vakken: Data Analysis Using R, Advanced bioinformatica. Projecten Project RNA sequencing: Het effect van BCLXL en ONECUT1 op de genexpressie van fibroblasten, Beschrijving: Gebruik van R, R packages zoals: DESeq2 en ggplot, en het uitvoeren van een GO term enrichment analysis. Project Metagenomics: Identificatie van bacteriesoorten Beschrijving: Identificeren van bacteriën in een monster met gebruik van Kraken2 en Bracken voor metagenomics analyse. Hogeschool Utrecht . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9/2021 – Heden Biologie en Medisch Laboratoriumonderzoek (Life Sciences) . . . . . . . Verwachte afstudeerdatum: 8/2025 Vakken: Data Science, Statistiek &amp; Excel. Projecten Project Data Science: Het effect van PFOS op de genexpressie van ACOX1, Beschrijving: Onderzoek naar het effect van PFOS op genexpressie, met gebruik van BASH voor data-manipulatie en JASP voor statistiek. Project Farmacon: Het effect van Donepezil op kunstmatig geïnduceerde paralyse in CL4176 C. elegans Beschrijving: Onderzoek naar het effect van Donepezil op amyloïde plaquevorming. Gebruik van de ingebouwde ImageJ macrofunctie om een klein script te schrijven voor de automatisering van data-analysestappen. Propedeuse Hogeschool Utrecht . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Behaald in 2022 Biologie en Medisch Laboratoriumonderzoek (Life Sciences) Minkema College (havo) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9/2015 – 7/2021 Profiel N&amp;G / N&amp;T MBO Rijnland . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9/2020 – 7/2021 Certificaat Biologie VWO 2.2 Werk Pizzeria Bella Milano, Woerden . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 04/2019 – Heden Opnemen en verwerken van bestellingen Bestellingen klaar maken en bezorgen Onderhouden van de website 2.3 Talen Nederlands (Moedertaal) Engels (Vloeiend) 2.4 Vaardigheden Programmeertalen R BASH Rapportage en Presentatie rmarkdown voor het maken van dynamische rapporten Data Analyse Ervaring met data-analyse en statistiek in R Gebruik van ggplot2 voor datavisualisatie Tekstmanipulatie Schrijven van BASH-scripts/pipelines Gebruik van AWK voor tekstmanipulatie Specifieke Libraries DESeq2: Differentiële expressie-analyse Kraken2: Identificatie van bacteriën in metagenomics tidyverse: Een verzameling van R-packages voor data science en data wrangling 2023-11-26 "],["looking-ahead.html", "Chapter 3 Looking ahead 3.1 Chapters and sub-chapters 3.2 Captioned figures and tables", " Chapter 3 Looking ahead Cross-references make it easier for your readers to find and link to elements in your book. 3.1 Chapters and sub-chapters There are two steps to cross-reference any heading: Label the heading: # Hello world {#nice-label}. Leave the label off if you like the automated heading generated based on your heading title: for example, # Hello world = # Hello world {#hello-world}. To label an un-numbered heading, use: # Hello world {-#nice-label} or {# Hello world .unnumbered}. Next, reference the labeled heading anywhere in the text using \\@ref(nice-label); for example, please see Chapter ??. If you prefer text as the link instead of a numbered reference use: any text you want can go here. 3.2 Captioned figures and tables Figures and tables with captions can also be cross-referenced from elsewhere in your book using \\@ref(fig:chunk-label) and \\@ref(tab:chunk-label), respectively. See Figure 3.1. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 3.1: Here is a nice figure! Don’t miss Table 3.1. knitr::kable( head(pressure, 10), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 3.1: Here is a nice table! temperature pressure 0 0.0002 20 0.0012 40 0.0060 60 0.0300 80 0.0900 100 0.2700 120 0.7500 140 1.8500 160 4.2000 180 8.8000 "],["looking-into-reproducible-science.html", "Chapter 4 Looking into Reproducible Science", " Chapter 4 Looking into Reproducible Science In this part we will look into reproducibilty of scientific publications, we will be doing this by comparing a paper to the criteria that is available on Researchgate, and by trying to recreate a part of the analysis from an different paper. Per‐ and polyfluoroalkyl substances activate UPR pathway, induce steatosis and fibrosis in liver cells https://doi.org/10.1002/tox.23680 Qi, Q., Niture, S., Gadi, S., Arthur, E., Moore, J., Levine, K. E., &amp; Kumar, D. (2023) The article researches how the retention of fat, fibrogenic signals and cell survival are influenced by low concentrations of three different PFAS in liver cell models. This is done using a handfull of techniques like cell counting and staining, RT qPCR, RNA-seq and others. Important findings are that PFAS in low concentrations can cause ER stress, steatosis and fibrogenic signaling, therefore associating PFAS to the development of non alcoholic fatty liver disease. We will score this paper on the following criteria: Study Purpose, Data Availability Statement, Data Location, Study Location, Author Review, Ethics Statement, Funding Statement and on Code Availability. This is done by filling in the table provided on Researchgate. The table seen beneath is the table with criteria filled in for the study we chosen above. Transparency Criteria Definition Response Type Score Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective. Binary no Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. Binary no Data Location Where the article’s data can be accessed, either raw or processed. Found Value ArrayExpress E‐MTAB‐11670, cell photographs are linked at the top under “Supplementary Materials” Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. Binary; Found Value no Author Review The professionalism of the contact information that the author has provided in the manuscript. Found Value Tier 3 Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. Binary no Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. Binary yes Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. Binary no Data Availablilty statement is score as a no, but at the beginning of the paper there is a small paragraph called Supplementary Materials where the cell photographs are linked. But there is no part about the RNA-seq data or the qPCR data. The RNA-seq data can be found hidden at the end of the methods. There is a clear statement about funding: Funding information National Institutes of Health, Grant/Award Numbers: R01MD012767, U54MD012392, U01CA194730. There is no ethics statement and nothing is said about code used. Hormonal correlates of pathogen disgust: testing the compensatory prophylaxis hypothesis https://doi.org/10.1016/j.evolhumbehav.2017.12.004. Benedict C. Jones, Amanda C. Hahn, Claire I. Fisher, Hongyi Wang, Michal Kandrik, Anthony J. Lee, Joshua M. Tybur, Lisa M. DeBruine(2018) The paper investigates if elevated progesterone during the menstrual cycle, leads to a higher pathogen disgust. The code and data used in this paper can be found here. We will first take a look at the code, all the code is stored in the OCMATE_disgust2.RMD file. First they load the data into R using read_csv, then they calculate some simple stats like mean age and how many session completed per woman. Then they clean up the data by removing points with missing values, excluding subjects with only one session completed and removing outliers. After that some plots and a lot of tables are formed. There is not really an introduction, they dive straight into loading libraries and importing code. There are some comments in the code but they are very basic and tell very little about the code. The code is structured in chunks with most chunks having a short header stating the goal of the code. Overall i would say that the readability of the code is a 2/5. Now we will try to run the Rmarkdown file ourselves, the first time trying to knit the Rmd we encountered some issues because two packages were not installed. We installed the packages using the following lines. install.packages(&quot;Matrix&quot;, dependencies = TRUE) install.packages(&quot;lme4&quot;, dependencies = TRUE) And after installing the packages the Rmd could be knitted, the entire html file can be seen here. But i will share a plot here as well. This is the first plot in the html file, very little is said about this plot except “# subject-mean-centre hormones # and divide by a constant to put all hormones on ~ -0.5 to +0.5 scale.” It took very little effort for me to reproduce the visualizations of this paper so I will rank it 5/5 on how much effort it took to reproduce. "],["sql.html", "Chapter 5 SQL 5.1 Getting the data 5.2 Tidy the data 5.3 Using SQL and R to inspect data 5.4 Summarzing flu and dengue 5.5 Merging the tables 5.6 Creating some plots", " Chapter 5 SQL N.B. some of the steps are done using an connection to a local postgres database, because these wouldn’t work in an R markdown they are replaced by screenshots. 5.1 Getting the data First we need to get the data for the exercise, we use wget to download them from github. # Go to data_raw cd data_raw/ # Download the flu dataset wget -O flu_data.csv https://raw.githubusercontent.com/DataScienceILC/tlsc-dsfb26v-20_workflows/main/data/flu_data.csv # Download the dengue dataset wget -O dengue_data.csv https://raw.githubusercontent.com/DataScienceILC/tlsc-dsfb26v-20_workflows/main/data/dengue_data.csv 5.2 Tidy the data Its important to make the data tidy so every table matches structure wise, in these steps we also make some changes to make merging later easier. #load needed libraries library(tidyr) library(dplyr) library(RPostgreSQL) library(dslabs) # Load the different data flu_data &lt;- read.csv(&quot;data_raw/flu_data.csv&quot;, skip = 11) # skip = 11 to skip the first 11 lines which contain metadata dengue_data &lt;- read.csv(&quot;data_raw/dengue_data.csv&quot;, skip = 11) gapminder_data &lt;- as_tibble(gapminder) ## flu_data # Make flu_data tidy tidy_flu_data &lt;- flu_data %&gt;% pivot_longer(cols = -Date, names_to = &quot;country&quot;, values_to = &quot;cases&quot;) # Split Date into year, month, day tidy_flu_data &lt;- tidy_flu_data %&gt;% separate(Date, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), sep = &quot;-&quot;) # Convert year into factor tidy_flu_data$year &lt;- as.factor(tidy_flu_data$year) # flu_data uses dots instead of spaces in country names (New.Zealand) # So to make joining the tables easier we need to replace the dots with spaces tidy_flu_data$country &lt;- gsub(&quot;\\\\.&quot;, &quot; &quot;, tidy_flu_data$country) # turn country into factor tidy_flu_data$country &lt;- as.factor(tidy_flu_data$country) # dengue_data # Make dengue_data tidy tidy_dengue_data &lt;- dengue_data %&gt;% pivot_longer(cols = -Date, names_to = &quot;country&quot;, values_to = &quot;cases&quot;) # Split Date into year, month, day tidy_dengue_data &lt;- tidy_dengue_data %&gt;% separate(Date, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), sep = &quot;-&quot;) # Convert year into factor tidy_dengue_data$year &lt;- as.factor(tidy_dengue_data$year) # turn country into factor tidy_dengue_data$country &lt;- as.factor(tidy_dengue_data$country) # Save tidy_dengue_data write.csv(tidy_dengue_data, &quot;data/tidy_dengue_data.csv&quot;) saveRDS(tidy_dengue_data, &quot;data/tidy_dengue_data.rds&quot;) # Save tidy_flu_data write.csv(tidy_flu_data, &quot;data/tidy_flu_data.csv&quot;) saveRDS(tidy_flu_data, &quot;data/tidy_flu_data.rds&quot;) # Save clean gapminder_data write.csv(gapminder_data, &quot;data/tidy_gapminder_data.csv&quot;) saveRDS(gapminder_data, &quot;data/tidy_gapminder_data.rds&quot;) The chunck beneath is to create an connection object, that can be used to connect to a Postgres database. # Load packages library(RPostgreSQL) library(DBI) # Create connection object to connect to local PostgreSQL database con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflowsdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;PASSWORD&quot;) This chunck would normaly create tables in the postgres database. # Load tidy_flu_data into Postgres database dbWriteTable(con, &quot;tidy_flu_data&quot;, as.data.frame(tidy_flu_data), overwrite = TRUE) # Load tidy_dengue_data into Postgres database dbWriteTable(con, &quot;tidy_dengue_data&quot;, as.data.frame(tidy_dengue_data), overwrite = TRUE) # Load gapminder_data into Postgres database dbWriteTable(con, &quot;gapminder_data&quot;, as.data.frame(gapminder_data), overwrite = TRUE) 5.3 Using SQL and R to inspect data We use both SQL and R to inspect the data in the same way 5.3.1 Gapminder Here we order the gapminder data by life expactancy. Using R with the same goal. gapminder_data %&gt;% arrange(desc(life_expectancy)) ## # A tibble: 10,545 × 9 ## country year infant_mortality life_expectancy fertility population gdp ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Hong Kong,… 2016 NA 83.9 NA NA NA ## 2 Hong Kong,… 2015 NA 83.7 1.17 7287983 NA ## 3 Hong Kong,… 2014 NA 83.6 1.15 7226869 NA ## 4 Hong Kong,… 2013 NA 83.4 1.14 7163930 NA ## 5 Iceland 2014 1.6 83.3 2.07 327318 NA ## 6 Iceland 2015 1.6 83.3 2.06 329425 NA ## 7 Iceland 2016 NA 83.3 NA NA NA ## 8 Japan 2016 NA 83.3 NA NA NA ## 9 Hong Kong,… 2012 NA 83.2 1.12 7101858 NA ## 10 Iceland 2013 1.6 83.2 2.08 325392 NA ## # … with 10,535 more rows, and 2 more variables: continent &lt;fct&gt;, region &lt;fct&gt; 5.3.2 Flu Here we select only the cases in France from the flu data. Again using R for the same goal. tidy_flu_data %&gt;% filter(tolower(country) == &#39;france&#39; &amp; !is.na(cases)) %&gt;% arrange(year, month, day) ## # A tibble: 620 × 5 ## year month day country cases ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;int&gt; ## 1 2003 09 28 France 25 ## 2 2003 10 05 France 25 ## 3 2003 10 12 France 30 ## 4 2003 10 19 France 81 ## 5 2003 10 26 France 82 ## 6 2003 11 02 France 48 ## 7 2003 11 09 France 146 ## 8 2003 11 16 France 274 ## 9 2003 11 23 France 877 ## 10 2003 11 30 France 1282 ## # … with 610 more rows 5.3.3 Dengue Here we sort dengue by cases from high to low. And again R for the same goal. tidy_dengue_data %&gt;% filter(!is.na(cases)) %&gt;% arrange(desc(cases)) ## # A tibble: 6,263 × 5 ## year month day country cases ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 2004 02 29 Indonesia 1 ## 2 2005 09 11 Singapore 1 ## 3 2008 04 06 Brazil 1 ## 4 2009 02 08 Bolivia 1 ## 5 2009 05 03 Argentina 1 ## 6 2009 09 27 Mexico 1 ## 7 2010 05 30 Venezuela 1 ## 8 2010 08 29 Philippines 1 ## 9 2012 10 21 India 1 ## 10 2013 07 14 Thailand 1 ## # … with 6,253 more rows Here you can see that in every country, the highest case is 1 because the data is relative per country. The day where there were the most Google searches about dengue is labeled as one, and the day with the fewest is labeled as 0. Every other day is on the scale between those. 5.4 Summarzing flu and dengue Both the flu and dengue data have datapoints every day, while gapminder has a datapoint for every year. Before we can join flu and dengue with gapminder we first have to summarize the data so flu and dengue to have a datapoint for every year. For flu we can just take the total of every case, hence we use sum in the summarize function. summarized_flu_data &lt;- tidy_flu_data %&gt;% mutate(Date = as.Date(paste(year, month, day, sep = &quot;-&quot;))) %&gt;% group_by(country, year) %&gt;% summarize(total_cases_flu = sum(cases, na.rm = TRUE)) %&gt;% ungroup() For dengue, we need to calculate the average because the data is on a scale where 1 represents the highest search activity in a country, and 0 represents the lowest. Therefore, we use the mean function in the summarize function. summarized_dengue_data &lt;- tidy_dengue_data %&gt;% mutate(Date = as.Date(paste(year, month, day, sep = &quot;-&quot;))) %&gt;% group_by(country, year) %&gt;% summarize(avg_cases_dengue = mean(cases, na.rm = TRUE)) %&gt;% ungroup() Now we load the new summarized files into the database. # Load summarized_flu_data dbWriteTable(con, &quot;summarized_flu_data&quot;, as.data.frame(summarized_flu_data), overwrite = TRUE) # Load summarized_dengue_data dbWriteTable(con, &quot;summarized_dengue_data&quot;, as.data.frame(summarized_dengue_data), overwrite = TRUE) 5.5 Merging the tables Normally this could be used to merge the tables and load the table into R. # SQL to merge tables sql_query &lt;- &quot; SELECT COALESCE(d.country, f.country) AS merged_country, COALESCE(d.year, f.year::text) AS merged_year, d.avg_cases_dengue, f.total_cases_flu, g.* FROM summarized_dengue_data AS d FULL JOIN summarized_flu_data AS f ON d.year = f.year::text AND d.country = f.country JOIN gapminder_data AS g ON COALESCE(d.year, f.year::text) = g.year::text AND COALESCE(d.country, f.country) = g.country ORDER BY COALESCE(d.avg_cases_dengue, 0) DESC, COALESCE(f.total_cases_flu, 0) DESC; &quot; # Export table to R merged_table &lt;- dbGetQuery(con, sql_query) summary(merged_table) Because we can’t use SQL we just load the merged table in using R, and create a summary of the data. merged_table &lt;- readRDS(&quot;data/merged_table.rds&quot;) summary(merged_table) ## merged_country merged_year avg_cases_dengue total_cases_flu ## Length:490 Length:490 Min. :0.0108 Min. : 0 ## Class :character Class :character 1st Qu.:0.0796 1st Qu.: 1696 ## Mode :character Mode :character Median :0.1216 Median : 6772 ## Mean :0.1363 Mean : 20147 ## 3rd Qu.:0.1772 3rd Qu.: 24975 ## Max. :0.5355 Max. :155577 ## NA&#39;s :360 NA&#39;s :84 ## country year infant_mortality life_expectancy ## Length:490 Min. :2002 Min. : 2.00 Min. :52.50 ## Class :character 1st Qu.:2005 1st Qu.: 4.00 1st Qu.:73.20 ## Mode :character Median :2008 Median : 7.60 Median :76.50 ## Mean :2008 Mean :12.61 Mean :75.84 ## 3rd Qu.:2012 3rd Qu.:15.88 3rd Qu.:80.50 ## Max. :2015 Max. :62.00 Max. :83.20 ## ## fertility population gdp continent ## Min. :1.150 Min. :3.324e+06 Min. :7.214e+09 Length:490 ## 1st Qu.:1.430 1st Qu.:9.241e+06 1st Qu.:8.430e+10 Class :character ## Median :1.850 Median :3.001e+07 Median :2.193e+11 Mode :character ## Mean :1.945 Mean :8.865e+07 Mean :7.992e+11 ## 3rd Qu.:2.288 3rd Qu.:8.051e+07 3rd Qu.:5.853e+11 ## Max. :3.980 Max. :1.311e+09 Max. :1.174e+13 ## NA&#39;s :141 ## region ## Length:490 ## Class :character ## Mode :character ## ## ## ## 5.6 Creating some plots We now use the merged table to create some plots to visualize the data. Create boxplot of life expectancy per continent. library(ggplot2) # Boxplot of Life Expectancy by Continent ggplot(merged_table, aes(x = continent, y = life_expectancy, fill = continent)) + geom_boxplot() + labs(title = &quot;Boxplot of Life Expectancy by Continent&quot;, x = &quot;Continent&quot;, y = &quot;Life Expectancy&quot;) + theme_minimal() + scale_fill_discrete(name = &quot;Continent&quot;) Linegraph for dengue cases in Brazil, Argentina and Bolivia. # chosen 3 random countries selected_countries &lt;- c(&quot;Brazil&quot;, &quot;Argentina&quot;, &quot;Bolivia&quot;) filtered_data_dengue &lt;- merged_table[merged_table$country %in% selected_countries, ] # Line plot of Dengue cases for Brazil, Argentina and Bolivia ggplot(filtered_data_dengue, aes(x = year, y = avg_cases_dengue, color = country)) + geom_line() + labs(title = &quot;Dengue Cases Over the Years (Selected Countries)&quot;, x = &quot;Year&quot;, y = &quot;Average Dengue Cases&quot;, color = &quot;Country&quot;) + theme_minimal() ## Warning: Removed 1 row(s) containing missing values (geom_path). Flu cases per capita per country. # Filter data to exclude NA values filtered_data &lt;- merged_table %&gt;% filter(!is.na(total_cases_flu) &amp; !is.na(population)) # Divide the cases by the population filtered_data$flu_ratio &lt;- filtered_data$total_cases_flu / filtered_data$population # Create a box plot ggplot(filtered_data, aes(x = merged_country, y = flu_ratio)) + geom_boxplot() + labs(title = &quot;Distribution of Flu Cases per Population per Country&quot;, x = &quot;Country&quot;, y = &quot;Flu Cases per Population&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Make readable "],["reproducing-an-analysis.html", "Chapter 6 Reproducing an Analysis 6.1 Getting the data 6.2 Normalizing the data 6.3 Visualizing the data 6.4 Normalizing the data again 6.5 Visualizing the data again", " Chapter 6 Reproducing an Analysis In this part we will be reproducing an analysis with data supplied by J. Louter (INT/ILC) Different compounds were tested on adult C.elegans and the amount of offspring were counted. 6.1 Getting the data First we will need to get the data from somewhere, we use wget to download the excel file from github. # Go to data_raw cd data_raw/ # Download the excel sheet dataset wget -O &quot;CE.LIQ.FLOW.062_Tidydata.xlsx&quot; https://github.com/DataScienceILC/tlsc-dsfb26v-20_workflows/raw/main/data/CE.LIQ.FLOW.062_Tidydata.xlsx Now we can read the file into R. # Load libraries library(readxl) library(dplyr) # Read the excel file C.elegans_data &lt;- read_excel(path = &quot;data_raw/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;) The file is now read into R. 6.2 Normalizing the data # Display the important rows C.elegans_data %&gt;% select(c(RawData, compName, compConcentration)) ## # A tibble: 360 × 3 ## RawData compName compConcentration ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 44 2,6-diisopropylnaphthalene 4.99 ## 2 37 2,6-diisopropylnaphthalene 4.99 ## 3 45 2,6-diisopropylnaphthalene 4.99 ## 4 47 2,6-diisopropylnaphthalene 4.99 ## 5 41 2,6-diisopropylnaphthalene 4.99 ## 6 35 2,6-diisopropylnaphthalene 4.99 ## 7 41 2,6-diisopropylnaphthalene 4.99 ## 8 36 2,6-diisopropylnaphthalene 4.99 ## 9 40 2,6-diisopropylnaphthalene 4.99 ## 10 38 2,6-diisopropylnaphthalene 4.99 ## # … with 350 more rows Here we see that compConcentration is of chr type, so we have to change it into a numeric type. # Load readr library(readr) # Use parse_number to change the conecentration into numeric C.elegans_data$compConcentration &lt;- parse_number(C.elegans_data$compConcentration) # Display the rows again C.elegans_data %&gt;% select(c(RawData, compName, compConcentration)) ## # A tibble: 360 × 3 ## RawData compName compConcentration ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 44 2,6-diisopropylnaphthalene 4.99 ## 2 37 2,6-diisopropylnaphthalene 4.99 ## 3 45 2,6-diisopropylnaphthalene 4.99 ## 4 47 2,6-diisopropylnaphthalene 4.99 ## 5 41 2,6-diisopropylnaphthalene 4.99 ## 6 35 2,6-diisopropylnaphthalene 4.99 ## 7 41 2,6-diisopropylnaphthalene 4.99 ## 8 36 2,6-diisopropylnaphthalene 4.99 ## 9 40 2,6-diisopropylnaphthalene 4.99 ## 10 38 2,6-diisopropylnaphthalene 4.99 ## # … with 350 more rows We can see the type is now of dbl type. 6.3 Visualizing the data # Load required libraries library(ggplot2) # Create scatterplot ggplot(C.elegans_data, aes(x = compConcentration, y = RawData, color = compName, shape = expType)) + geom_point() + labs(x = &quot;Compound Concentration (nM)&quot;, y = &quot;Raw Data Counts&quot;) ## Warning: Removed 5 rows containing missing values (geom_point). The graph is really cramped into the left part because of a couple of very high concentrations. This occurs because the concentrations are created by diluting the previous solution by 1/10. An obvious fix for this is to use an log10 transformation on the x-axis. # Create plot, note that we added + 0.01 this is becasue the log10 of 0 is undefined. # So adding 0.01 makes sure that points where concentration is 0 (controlNegative) show up in the graph ggplot(C.elegans_data, aes(x = log10(compConcentration + 0.01), y = RawData, color = compName, shape = expType)) + geom_point(position = position_jitter(width = 0.2, height = 0.2)) + # Add jitter labs(x = &quot;Log10(Compound Concentration (nM))&quot;, y = &quot;Raw Data Counts&quot;) ## Warning: Removed 5 rows containing missing values (geom_point). This looks way better, we also added some jitter to prevent points from overlapping. 6.4 Normalizing the data again We have put nanomolar(nM) on the x axis but not all the data is in nM some points are in percentage(pct) as shown below #show that both nM and pct are used C.elegans_data$compUnit %&gt;% unique() ## [1] &quot;nM&quot; &quot;pct&quot; Lets take a look to see where pct is being used. # Load DT library(DT) # Select rows where compUnit is pct and select the more important columns C.elegans_data %&gt;% filter(compUnit == &quot;pct&quot;) %&gt;% select(c(compName, expType, RawData, compConcentration, compUnit)) %&gt;% datatable(options = list(pageLength = 5)) # Show as datatable As you can see in the interactive table both the positive control and the negative control, as well as the vehicle control, are in percentage (pct). Since the concentration (pct) of the negative control is 0 (nothing added) we can ignore the negative control and focus on the other two, these are both ethanol solution with either 1,5% or 0,5% ethanol. Both the positive control and the control Vehicle A are in percentage ethanol solution. We can use a formula to transfer these to nM like the rest of the data. \\[nM=(x*10^7)/46,07\\] Where x is the percentage ethanol. Here we use this formula to change the percentages to molarity but its important to do this only in the rows where the compName is ethanol and the compunit pct. # Change compConcentration where compName is Ethanol and compUnit pct C.elegans_data$compConcentration &lt;- ifelse(C.elegans_data$compName == &quot;Ethanol&quot; &amp; C.elegans_data$compUnit == &quot;pct&quot;, (C.elegans_data$compConcentration * 10^7) / 46.07, C.elegans_data$compConcentration) Now that we changed the values from percentage to nM we can visualize the data again. 6.5 Visualizing the data again Now the data is acctually normalized we can finally visualize the data so we create a scatterplot again. ggplot(C.elegans_data, aes(x = log10(compConcentration + 0.01), y = RawData, color = compName, shape = expType)) + geom_point(position = position_jitter(width = 0.2, height = 0.2)) + # Add jitter labs(x = &quot;Log10(Compound Concentration)&quot;, y = &quot;Raw Data Counts&quot;) ## Warning: Removed 5 rows containing missing values (geom_point). We also create a faceted graph to make it a bit easier to see the different controls. # Create a faceted scatterplot with log-transformed x-axis and jitter using crosses ggplot(C.elegans_data, aes(x = log10(compConcentration + 0.01), y = RawData, color = compName)) + geom_point(shape = &quot;+&quot;, size = 3, position = position_jitter(width = 0.2, height = 0.2)) + labs(x = &quot;Log10(Compound Concentration)&quot;, y = &quot;Raw Data Counts&quot;) + facet_wrap(~expType, scales = &quot;fixed&quot;, ncol = 2) + # split graph in 4 for every exptype theme_minimal() ## Warning: Removed 5 rows containing missing values (geom_point). The negative control for this experiment is S-medium 0 nM so basically nothing is added, and the positive control is a high concentration ethanol. Think about how you would analyze this experiment to learn whether there is indeed an effect of different concentrations on offspring count and whether the different compounds have a different curve (IC50). Write down you analysis as a step-wise plan. Normalize data so the mean of the negative control is 1. # Find the mean of controlNegative controlNegative_mean &lt;- mean(C.elegans_data$RawData[C.elegans_data$expType == &quot;controlNegative&quot;], na.rm = TRUE) # Use mean to nomralize data C.elegans_data &lt;- C.elegans_data %&gt;% mutate(Normalized_RawData = RawData / controlNegative_mean) ggplot(C.elegans_data, aes(x = log10(compConcentration + 0.01), y = Normalized_RawData, color = compName, shape = expType)) + geom_point(position = position_jitter(width = 0.2, height = 0.2)) + labs(x = &quot;Log10(Compound Concentration)&quot;, y = &quot;Normalized Raw Data&quot;) ## Warning: Removed 5 rows containing missing values (geom_point). "],["blocks.html", "Chapter 7 Blocks 7.1 Equations 7.2 Theorems and proofs 7.3 Callout blocks", " Chapter 7 Blocks 7.1 Equations Here is an equation. \\[\\begin{equation} f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k} \\tag{7.1} \\end{equation}\\] You may refer to using \\@ref(eq:binom), like see Equation (7.1). 7.2 Theorems and proofs Labeled theorems can be referenced in text using \\@ref(thm:tri), for example, check out this smart theorem 7.1. Theorem 7.1 For a right triangle, if \\(c\\) denotes the length of the hypotenuse and \\(a\\) and \\(b\\) denote the lengths of the other two sides, we have \\[a^2 + b^2 = c^2\\] Read more here https://bookdown.org/yihui/bookdown/markdown-extensions-by-bookdown.html. 7.3 Callout blocks The R Markdown Cookbook provides more help on how to use custom blocks to design your own callouts: https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html "],["references.html", "References", " References "],["covid-report.html", "Chapter 8 Covid report", " Chapter 8 Covid report "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
